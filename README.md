# Examining eHMIs in critical driver-pedestrian encounters in a coupled simulator

Based on the open-source [coupled sim](https://github.com/bazilinskyy/coupled-sim). We welcome forking of this repository, pull requests, and any contributions in the spirit of open science and open-source code ðŸ˜ðŸ˜„ For enquiries about collaboration, you may contact p.bazilinskyy@tue.nl.

### Citation
If you use results of the experiment conducted with the setup for academic work please cite the following paper:

> Mok, C. S., Bazilinskyy, P., & De Winter, J. C. F. (2022). Stopping by looking: A driver-pedestrian interaction study in a coupled simulator using head-mounted displays with eye-tracking. Applied Ergonomics, 105, 103825. http://doi.org/10.1016/j.apergo.2022.103825

## Description of the experiment
:tv: The image below points to a YouTube video of analysis of the experimental results:

[![demo video](ReadmeFiles/thumbnail_demo_video.jpg)](https://youtu.be/s1Ww_vTWn8k)

Additionally, there are three more videos which demonstrate the setup:
1. LTY (Look to Yield) trial in driver-pedestrian interaction study in a coupled simulator with HMD with eye-tracking [https://youtube.com/shorts/-N8f-2EhPtg](https://youtube.com/shorts/-N8f-2EhPtg)
2. LATY (Look Away to Yield) trial in driver-pedestrian interaction study in a coupled simulator with HMD with eye-tracking [https://youtube.com/shorts/1VCyx8oC0SA](https://youtube.com/shorts/1VCyx8oC0SA)

Automated vehicles (AVs) can perform low-level control tasks but are not always capable of proper decision-making. This paper presents a concept of eye-based maneuver control for AV-pedestrian interaction. Previously, it was unknown whether the AV should conduct a stopping maneuver when the driver looks at the pedestrian or looks away from the pedestrian. A two-agent experiment was conducted using two head-mounted displays with integrated eye-tracking. Seventeen pairs of participants (pedestrian and driver) each interacted in a road crossing scenario. The pedestrians' task was to hold a button when they felt safe to cross the road, and the drivers' task was to direct their gaze according to instructions. Participants completed three 16-trial blocks: (1) Baseline, in which the AV was pre-programmed to yield or not yield, (2) Look to Yield (LTY), in which the AV yielded when the driver looked at the pedestrian, and (3) Look Away to Yield (LATY), in which the AV yielded when the driver did not look at the pedestrian. The driver's eye movements in the LTY and LATY conditions were visualized using a virtual light beam. Crossing performance was assessed based on whether the pedestrian held the button when the AV yielded and released the button when the AV did not yield. Furthermore, the pedestrians' and drivers' acceptance of the mappings was measured through a questionnaire. The results showed that the LTY and LATY mappings yielded better crossing performance than Baseline. Furthermore, the LTY condition was best accepted by drivers and pedestrians. Eye-tracking analyses indicated that the LTY and LATY mappings attracted the pedestrian's attention, while pedestrians still distributed their attention between the AV and a second vehicle approaching from the other direction. In conclusion, LTY control may be a promising means of AV control at intersections before full automation is technologically feasible.

## How to run
Check instructions at the repo of the [coupled sim](https://github.com/bazilinskyy/coupled-sim#how-to-run).



